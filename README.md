# Finviz-Webscraper-Tutorial
Given a finviz filter url, we can use a python program to webscrape a list of stock tickers from it.
This tutorial assumes you have python set up and have a basic understanding of how to use it.

This python program depends on the Selenium and Webdriver Manager libraries to run. 
Those can be installed with the pip commands or with conda.
>pip install selenium

>pip install webdriver_manager

If you get any error messages with the pip command that could be due to your python version. Try "pip3" instead.


<img src="https://github.com/denged1/Finviz-Webscraper/blob/main/docs/screenerSetUp.png" style=" width:750px ; height:450px "  >

First start by creating or finding a screener in finviz. Here I selected any company in the technology sector that pays out a 3% of higher dividend.
Since finviz screeners parameters are saved into the url, the tutorial and code will work for any finviz screener.


<!---
![alt text](https://github.com/denged1/Finviz-Webscraper/blob/main/docs/screenerSetUp.png?raw=true "Set Up")
-->

<img src="https://github.com/denged1/Finviz-Webscraper/blob/main/docs/screenerInspect.png" style=" width:750px ; height:450px "  >


Now that we have our generated screener we must now also see how many pages of tickers are generated as well. At first this might seem uneccesary, but if we were to start scraping the tickers without iterating through each page the screener generates, we only ever get the first 25 tickers. However, if we get a list of urls and iterate through them, scraping every ticker before moving to the next, we can ensure we will get every ticker.

To find which web element describes the last page, which in the screenshot is page 40, simply right click and press "Inspect".
In our case, the last page will always be given by the last element with class='screener-pages'. In this screenshot I picked the set of all 
technology stocks to demonstrate that this is true even for larger screeer sets.

Using this information, we can now instantiate a webdriver object, navigate to the url and collect the page numbers.
If you have the ChromeDriver downloaded and are not using Webdriver Manager, use
>driver = webdriver.Chrome()

instead and it will automatically search for the path to it. (ChromeDriver can be downloaded at this url https://chromedriver.chromium.org/home)

```python
url = "https://finviz.com/screener.ashx?v=111&f=fa_div_o3,sec_technology"
chrome = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=chrome)
driver.get(url)

pages = driver.find_elements(By.CLASS_NAME, 'screener-pages')
#since we now have a list of the page numbers shown, if we navigate to the last item in the list, we have the last page number
last_page = int(pages[-1].text)
```
Now that we the last page number, we can make a for loop to append every page url generate by the screener. This is because finviz urls have a nice property that almost every url generated by a screener is the same except for the number at the end. The first page generated is unique, but luckily, we already have the orginal url. Thus we can write the following code to generate a list of urls for every single page number.

```python
url_list = [url]
for i in range(1,last_page):
    text = '&r=' + str(i * 20 +1)
    url_list.append(url+text)
```

<img src="https://github.com/denged1/Finviz-Webscraper/blob/main/docs/tickerInspect.png" style=" width:750px ; height:450px "  >


Next we need to identify the class that describes the tickers. Once again we right click on the ticker and find the class name that describes it.
In this case, the ticker is given by class="screener-link-primary".
Similarly, we can now use find elements to access the ticker names. Now we can create a for loop and add each ticker on the page to a list.

```python
symbols = driver.find_elements(By.CLASS_NAME, 'screener-link-primary')
ticker_list = []
for i in symbols:
    #the objects in the symbols list are selenium objects, so you need to use the .text function to convert to string
    ticker_list.append(i.text)
```
Now we have all the elements we fully iterate through all the pages, we can put it all together. The complete code can also be found in the respository.

```python
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service


#declaring a variable to store the url
urls = "https://finviz.com/screener.ashx?v=111&f=fa_div_o3,sec_technology"

#this function gets the urls for all the pages, and returns a list of urls
def get_page_urls(url):
    #instantiate a chrome driver object
    chrome = Service(ChromeDriverManager().install())
    #pass the chrome driver object to the webdriver.Chrome() function to instantiate a webdriver object
    driver = webdriver.Chrome(service=chrome)
    driver.get(url)
    #this will differ by page, right click on the element you want to get and click inspect, then copy the class name
    pages = driver.find_elements(By.CLASS_NAME, 'screener-pages')
    #this essentially found the last page number on a page to set up the for loop
    last_page = int(pages[-1].text)
    url_list = [url]
    for i in range(1,last_page):
        #this is different per page as well, in my case the url is the same except for the number at the end
        text = '&r=' + str(i * 20 +1)
        url_list.append(url+text)
    return url_list

def get_ticker_symbols(url):
    #want a service type object, you can put "ChromeDriverManager().install()" directly in the webdriver.Chrome() function, but will
    #pass a deprecated warning
    chrome = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=chrome)
    driver.get(url)
    ################################### IMPORTANT: DON"T MISSPELL THE FUNCTION ########################################
    #use find_element to get the first element instead, this returns a list of the elements
    symbols = driver.find_elements(By.CLASS_NAME, 'screener-link-primary')

    #this function actually gets the ticker symbols from each page, iterating through the symbols list
    ticker_list = []
    for i in symbols:
        #the objects in the symbols list are selenium objects, so you need to use the .text function to convert to string
        ticker_list.append(i.text)
    #
    #print(ticker_list)
    return ticker_list

def get_tickers(url):
    #get the urls for all the pages
    url_list = get_page_urls(url)
    #get the ticker symbols for all the pages
    ticker_list = []
    for i in url_list:
        ticker_list += get_ticker_symbols(i)
    return ticker_list


print(get_tickers(urls))


```

